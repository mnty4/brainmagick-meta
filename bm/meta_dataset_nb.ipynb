{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hostname lukas-B550-AORUS-ELITE-AX-V2 not defined in /conf/study_paths/study_paths.yaml. Using default paths.\n",
      "/home/lukas/miniforge3/envs/lavis-bm3/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Hostname lukas-B550-AORUS-ELITE-AX-V2 not defined in /conf/study_paths/study_paths.yaml. Using default paths.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lukas/projects/brainmagick\n",
      "hello there good sir.\n",
      "Opening raw data file /home/lukas/projects/brainmagick/cache/studies/gwilliams2022/01_session0_story0/meg-sr120-hp0-raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 47519 =      0.000 ...   395.992 secs\n",
      "Ready.\n",
      "Opening raw data file /home/lukas/projects/brainmagick/cache/studies/gwilliams2022/01_session0_story1/meg-sr120-hp0-raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 85919 =      0.000 ...   715.992 secs\n",
      "Ready.\n",
      "[Gwilliams2022Recording('01_session0_story0'), Gwilliams2022Recording('01_session0_story1')]\n",
      "Opening raw data file /home/lukas/projects/brainmagick/cache/studies/gwilliams2022/01_session0_story0/meg-sr128-hp0-raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 50687 =      0.000 ...   395.992 secs\n",
      "Ready.\n",
      "Reading 0 ... 50687  =      0.000 ...   395.992 secs...\n",
      "Opening raw data file /home/lukas/projects/brainmagick/cache/studies/gwilliams2022/01_session0_story1/meg-sr128-hp0-raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 91647 =      0.000 ...   715.992 secs\n",
      "Ready.\n",
      "Reading 0 ... 91647  =      0.000 ...   715.992 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "0it [00:06, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m initialize(version_base\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.1\u001b[39m\u001b[38;5;124m\"\u001b[39m, config_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconf\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     16\u001b[0m     cfg \u001b[38;5;241m=\u001b[39m compose(config_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, overrides\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m+HYDRA_FULL_ERROR=1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 17\u001b[0m     raws, events, infos \u001b[38;5;241m=\u001b[39m \u001b[43mmeta_preprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/brainmagick/bm/meta_preprocess.py:314\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    312\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCaching intermediate data under \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mcache\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    313\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(args)\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_BM_TEST_PATH\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    318\u001b[0m     main\u001b[38;5;241m.\u001b[39mdora\u001b[38;5;241m.\u001b[39mdir \u001b[38;5;241m=\u001b[39m Path(os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_BM_TEST_PATH\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/projects/brainmagick/bm/meta_preprocess.py:296\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    292\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextra_test_features\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWordHash\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# return get_raw_events(**kwargs, num_workers=args.num_workers)\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# return preprocess_words_test(**kwargs, num_workers=args.num_workers)\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpreprocess_words\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/brainmagick/bm/meta_preprocess.py:172\u001b[0m, in \u001b[0;36mpreprocess_words\u001b[0;34m(save_dir, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(word_index_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    170\u001b[0m         word_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(f, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 172\u001b[0m subs, word_index, additional_info \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_recordings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraws\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSaving tensors to \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sub \u001b[38;5;129;01min\u001b[39;00m subs:\n",
      "File \u001b[0;32m~/projects/brainmagick/bm/meta_preprocess.py:239\u001b[0m, in \u001b[0;36mpreprocess_recordings\u001b[0;34m(raws, events, infos, word_index, offset, n_fft, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m     skipped \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m spectrums: Spectrum \u001b[38;5;241m=\u001b[39m \u001b[43mraw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_psd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# spectrums.plot(picks=\"data\", exclude=\"bads\", amplitude=False)\u001b[39;00m\n\u001b[1;32m    241\u001b[0m data, freqs \u001b[38;5;241m=\u001b[39m spectrums\u001b[38;5;241m.\u001b[39mget_data(return_freqs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m<decorator-gen-176>:10\u001b[0m, in \u001b[0;36mcompute_psd\u001b[0;34m(self, method, fmin, fmax, tmin, tmax, picks, exclude, proj, remove_dc, reject_by_annotation, n_jobs, verbose, **method_kw)\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/lavis-bm3/lib/python3.8/site-packages/mne/io/base.py:2184\u001b[0m, in \u001b[0;36mBaseRaw.compute_psd\u001b[0;34m(self, method, fmin, fmax, tmin, tmax, picks, exclude, proj, remove_dc, reject_by_annotation, n_jobs, verbose, **method_kw)\u001b[0m\n\u001b[1;32m   2181\u001b[0m method \u001b[38;5;241m=\u001b[39m _validate_method(method, \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   2182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_legacy_nfft_default(tmin, tmax, method, method_kw)\n\u001b[0;32m-> 2184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSpectrum\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2185\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpicks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpicks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_dc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_dc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreject_by_annotation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreject_by_annotation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2198\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmethod_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2199\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/lavis-bm3/lib/python3.8/site-packages/mne/time_frequency/spectrum.py:1147\u001b[0m, in \u001b[0;36mSpectrum.__init__\u001b[0;34m(self, inst, method, fmin, fmax, tmin, tmax, picks, exclude, proj, remove_dc, reject_by_annotation, n_jobs, verbose, **method_kw)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;66;03m# do the basic setup\u001b[39;00m\n\u001b[0;32m-> 1147\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43minst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpicks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_dc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmethod_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;66;03m# get just the data we want\u001b[39;00m\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minst, BaseRaw):\n",
      "File \u001b[0;32m~/miniforge3/envs/lavis-bm3/lib/python3.8/site-packages/mne/time_frequency/spectrum.py:353\u001b[0m, in \u001b[0;36mBaseSpectrum.__init__\u001b[0;34m(self, inst, method, fmin, fmax, tmin, tmax, picks, exclude, proj, remove_dc, n_jobs, verbose, **method_kw)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# prep times and picks\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time_mask \u001b[38;5;241m=\u001b[39m _time_mask(inst\u001b[38;5;241m.\u001b[39mtimes, tmin, tmax, sfreq\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msfreq)\n\u001b[0;32m--> 353\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_picks \u001b[38;5;241m=\u001b[39m \u001b[43m_picks_to_idx\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpicks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_ref_meg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# add the info object. bads and non-data channels were dropped by\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# _picks_to_idx() so we update the info accordingly:\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo \u001b[38;5;241m=\u001b[39m pick_info(inst\u001b[38;5;241m.\u001b[39minfo, sel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_picks, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniforge3/envs/lavis-bm3/lib/python3.8/site-packages/mne/_fiff/pick.py:1271\u001b[0m, in \u001b[0;36m_picks_to_idx\u001b[0;34m(info, picks, none, exclude, allow_empty, with_ref_meg, return_kind, picks_on)\u001b[0m\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpicks must be 1D, got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (picks\u001b[38;5;241m.\u001b[39mndim,))\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m picks\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mchar \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mU\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1271\u001b[0m     picks \u001b[38;5;241m=\u001b[39m \u001b[43m_picks_str_to_idx\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpicks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_ref_meg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_kind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_repr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[43morig_picks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_kind:\n\u001b[1;32m   1282\u001b[0m         picked_ch_type_or_generic \u001b[38;5;241m=\u001b[39m picks[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/lavis-bm3/lib/python3.8/site-packages/mne/_fiff/pick.py:1337\u001b[0m, in \u001b[0;36m_picks_str_to_idx\u001b[0;34m(info, picks, exclude, with_ref_meg, return_kind, extra_repr, allow_empty, orig_picks)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     picks_generic \u001b[38;5;241m=\u001b[39m pick_channels(\n\u001b[1;32m   1334\u001b[0m         info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mch_names\u001b[39m\u001b[38;5;124m\"\u001b[39m], info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mch_names\u001b[39m\u001b[38;5;124m\"\u001b[39m], exclude\u001b[38;5;241m=\u001b[39muse_exclude\n\u001b[1;32m   1335\u001b[0m     )\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m picks[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1337\u001b[0m     picks_generic \u001b[38;5;241m=\u001b[39m \u001b[43m_pick_data_channels\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1338\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_ref_meg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_ref_meg\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m picks[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_or_ica\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1341\u001b[0m     picks_generic \u001b[38;5;241m=\u001b[39m _pick_data_or_ica(info, exclude\u001b[38;5;241m=\u001b[39mexclude)\n",
      "File \u001b[0;32m~/miniforge3/envs/lavis-bm3/lib/python3.8/site-packages/mne/_fiff/pick.py:1202\u001b[0m, in \u001b[0;36m_pick_data_channels\u001b[0;34m(info, exclude, with_ref_meg, with_aux)\u001b[0m\n\u001b[1;32m   1200\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m   1201\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(eog\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, ecg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, emg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, bio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 1202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpick_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref_meg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_ref_meg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/lavis-bm3/lib/python3.8/site-packages/mne/_fiff/pick.py:507\u001b[0m, in \u001b[0;36mpick_types\u001b[0;34m(info, meg, eeg, stim, eog, ecg, emg, ref_meg, misc, resp, chpi, exci, ias, syst, seeg, dipole, gof, bio, ecog, fnirs, csd, dbs, temperature, gsr, eyetrack, include, exclude, selection)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;66;03m# NOTE: Changes to this function's signature should also be changed in\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m# PickChannelsMixin\u001b[39;00m\n\u001b[1;32m    505\u001b[0m _validate_type(meg, (\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 507\u001b[0m exclude \u001b[38;5;241m=\u001b[39m \u001b[43m_check_info_exclude\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m nchan \u001b[38;5;241m=\u001b[39m info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnchan\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    509\u001b[0m pick \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(nchan, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/lavis-bm3/lib/python3.8/site-packages/mne/_fiff/pick.py:445\u001b[0m, in \u001b[0;36m_check_info_exclude\u001b[0;34m(info, exclude)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_info_exclude\u001b[39m(info, exclude):\n\u001b[1;32m    444\u001b[0m     _validate_type(info, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 445\u001b[0m     \u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_consistency\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exclude \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexclude must be a list of strings or \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbads\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/lavis-bm3/lib/python3.8/site-packages/mne/_fiff/meas_info.py:1820\u001b[0m, in \u001b[0;36mInfo._check_consistency\u001b[0;34m(self, prepend_error)\u001b[0m\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;66;03m# Ensure info['chs'] has immutable entries (copies much faster)\u001b[39;00m\n\u001b[1;32m   1819\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ci, ch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchs\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m-> 1820\u001b[0m     \u001b[43m_check_ch_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43mch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mci\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1821\u001b[0m     ch_name \u001b[38;5;241m=\u001b[39m ch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mch_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1822\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ch_name, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/miniforge3/envs/lavis-bm3/lib/python3.8/site-packages/mne/_fiff/meas_info.py:937\u001b[0m, in \u001b[0;36m_check_ch_keys\u001b[0;34m(ch, ci, name, check_min)\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_ch_keys\u001b[39m(ch, ci, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo[\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchs\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m, check_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 937\u001b[0m     ch_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    938\u001b[0m     bad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(ch_keys\u001b[38;5;241m.\u001b[39mdifference(_ALL_CH_KEYS_SET))\n\u001b[1;32m    939\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bad:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from bm import meta_dataset\n",
    "from bm import meta_preprocess\n",
    "from pathlib import Path\n",
    "import bm\n",
    "import sys\n",
    "import os\n",
    "from hydra import initialize, compose\n",
    "import hydra\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "\n",
    "sys.argv=['self.py']\n",
    "\n",
    "os.chdir(Path(bm.__file__).parent.parent)\n",
    "print(os.getcwd())\n",
    "with initialize(version_base=\"1.1\", config_path=\"conf\"):\n",
    "    cfg = compose(config_name=\"config.yaml\", overrides=['+HYDRA_FULL_ERROR=1'])\n",
    "    raws, events, infos = meta_preprocess.main(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukas/anaconda3/envs/brainmagick/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2Processor\n",
    "import librosa\n",
    "\n",
    "# processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\")\n",
    " # model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\", output_hidden_states=True).to(device)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\").to(device)\n",
    "\n",
    "\n",
    "def get_audio_label(wav_path, start, duration):\n",
    "    data_dir = Path('data/gwilliams2022/download')\n",
    "    audio, rate = librosa.load(os.path.join(data_dir, wav_path), sr = 16000, offset=start, duration=duration)\n",
    "    \n",
    "    # Process the audio to extract input features\n",
    "    input_features = processor(audio, sampling_rate=16000, return_tensors=\"pt\").input_values.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_features)\n",
    "\n",
    "    audio_embeddings = outputs.last_hidden_state\n",
    "    return audio_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from bm import speech_embeddings\n",
    "\n",
    "generate_embeddings = speech_embeddings.SpeechEmbeddings(**cfg)\n",
    "\n",
    "def get_audio_label(*args, **kwargs):\n",
    "    return generate_embeddings.get_audio_embeddings(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lukas/projects/brainmagick\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3134 entries, 0 to 3133\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype         \n",
      "---  ------       --------------  -----         \n",
      " 0   onset        3134 non-null   datetime64[ns]\n",
      " 1   duration     3134 non-null   float64       \n",
      " 2   description  3134 non-null   object        \n",
      "dtypes: datetime64[ns](1), float64(1), object(1)\n",
      "memory usage: 73.6+ KB\n",
      "Effective window size : 0.120 (s)\n",
      "word: Tara, audio features: torch.Size([14, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: stood, audio features: torch.Size([11, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: stock, audio features: torch.Size([18, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: still, audio features: torch.Size([19, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: waiting, audio features: torch.Size([20, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: for, audio features: torch.Size([6, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: first, audio features: torch.Size([13, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: tiny, audio features: torch.Size([15, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: gleam, audio features: torch.Size([11, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43143/1088313681.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print(f'word: {word_label}, audio features: {torch.tensor(audio_label).shape}, PSD: {torch.tensor(data).shape}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: from, audio features: torch.Size([8, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: scout, audio features: torch.Size([14, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: craft, audio features: torch.Size([14, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: to, audio features: torch.Size([7, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: appear, audio features: torch.Size([13, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: darkness, audio features: torch.Size([20, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: of, audio features: torch.Size([6, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: The, audio features: torch.Size([6, 768]), PSD: torch.Size([208, 8])\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6622 entries, 0 to 6621\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype         \n",
      "---  ------       --------------  -----         \n",
      " 0   onset        6622 non-null   datetime64[ns]\n",
      " 1   duration     6622 non-null   float64       \n",
      " 2   description  6622 non-null   object        \n",
      "dtypes: datetime64[ns](1), float64(1), object(1)\n",
      "memory usage: 155.3+ KB\n",
      "Effective window size : 0.120 (s)\n",
      "word: The, audio features: torch.Size([6, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: Cable, audio features: torch.Size([23, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: Fort, audio features: torch.Size([20, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: by, audio features: torch.Size([11, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: Bill, audio features: torch.Size([12, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: Glover, audio features: torch.Size([29, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: Hey, audio features: torch.Size([25, 768]), PSD: torch.Size([208, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43143/1088313681.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print(f'word: {word_label}, audio features: {torch.tensor(audio_label).shape}, PSD: {torch.tensor(data).shape}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective window size : 0.120 (s)\n",
      "word: Roy, audio features: torch.Size([27, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: What, audio features: torch.Size([10, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: You, audio features: torch.Size([10, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: suck, audio features: torch.Size([19, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: Chad, audio features: torch.Size([15, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: said, audio features: torch.Size([17, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: He, audio features: torch.Size([8, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: wished, audio features: torch.Size([14, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: Roy, audio features: torch.Size([10, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: fall, audio features: torch.Size([12, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: for, audio features: torch.Size([8, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: that, audio features: torch.Size([11, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: gag, audio features: torch.Size([12, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: every, audio features: torch.Size([12, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: time, audio features: torch.Size([23, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: get, audio features: torch.Size([8, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: me, audio features: torch.Size([8, 768]), PSD: torch.Size([208, 8])\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11597 entries, 0 to 11596\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype         \n",
      "---  ------       --------------  -----         \n",
      " 0   onset        11597 non-null  datetime64[ns]\n",
      " 1   duration     11597 non-null  float64       \n",
      " 2   description  11597 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(1), object(1)\n",
      "memory usage: 271.9+ KB\n",
      "Effective window size : 0.120 (s)\n",
      "word: Easy, audio features: torch.Size([14, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: Money, audio features: torch.Size([15, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: by, audio features: torch.Size([9, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: Bill, audio features: torch.Size([11, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: Glover, audio features: torch.Size([27, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: Charles, audio features: torch.Size([17, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43143/1088313681.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print(f'word: {word_label}, audio features: {torch.tensor(audio_label).shape}, PSD: {torch.tensor(data).shape}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: Acres, audio features: torch.Size([25, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: called, audio features: torch.Size([18, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: me, audio features: torch.Size([9, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: on, audio features: torch.Size([6, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: Thursday, audio features: torch.Size([21, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: afternoon, audio features: torch.Size([36, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: just, audio features: torch.Size([19, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: like, audio features: torch.Size([8, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: seen, audio features: torch.Size([14, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: him, audio features: torch.Size([7, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: since, audio features: torch.Size([15, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: high, audio features: torch.Size([25, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: school, audio features: torch.Size([27, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: maybe, audio features: torch.Size([21, 768]), PSD: torch.Size([208, 8])\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17999 entries, 0 to 17998\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype         \n",
      "---  ------       --------------  -----         \n",
      " 0   onset        17999 non-null  datetime64[ns]\n",
      " 1   duration     17999 non-null  float64       \n",
      " 2   description  17999 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(1), object(1)\n",
      "memory usage: 422.0+ KB\n",
      "Effective window size : 0.120 (s)\n",
      "word: Black, audio features: torch.Size([15, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: Willow, audio features: torch.Size([19, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: Allan, audio features: torch.Size([14, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: sat, audio features: torch.Size([11, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: down, audio features: torch.Size([10, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43143/1088313681.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print(f'word: {word_label}, audio features: {torch.tensor(audio_label).shape}, PSD: {torch.tensor(data).shape}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: his, audio features: torch.Size([6, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: desk, audio features: torch.Size([13, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: pulled, audio features: torch.Size([10, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: chair, audio features: torch.Size([14, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: close, audio features: torch.Size([19, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: legs, audio features: torch.Size([15, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: love, audio features: torch.Size([8, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: forced, audio features: torch.Size([16, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: windows, audio features: torch.Size([16, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: leaps, audio features: torch.Size([12, 768]), PSD: torch.Size([208, 8])\n",
      "Effective window size : 0.120 (s)\n",
      "word: imagination, audio features: torch.Size([35, 768]), PSD: torch.Size([208, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from mne.io import Raw\n",
    "from mne.time_frequency import Spectrum\n",
    "import julius\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# events[1][events[1][\"kind\"] == 'word']\n",
    "\n",
    "# print(raws[0].annotations[-1])\n",
    "\n",
    "# print(raws[0][:,52.12:])\n",
    "\n",
    "# raw = raws[0]\n",
    "# event = events[0][events[0][\"kind\"] == 'word']\n",
    "# word_event = event.iloc[0]\n",
    "# start = word_event['start']\n",
    "# end = start + word_event['duration']\n",
    "# print(start, end)\n",
    "# t_idx = raw.time_as_index([start, end])\n",
    "# data, times = raw[:,t_idx[0]:t_idx[1]]\n",
    "# print(event)\n",
    "# print(torch.tensor(data).shape)\n",
    "\n",
    "\n",
    "print(os.getcwd())\n",
    "# X = []\n",
    "# Y = []\n",
    "# word_labels = []\n",
    "subs = defaultdict(list)\n",
    "for raw, event, info in zip(raws, events, infos):\n",
    "    raw: Raw\n",
    "    event: pd.DataFrame\n",
    "     \n",
    "    word_events: pd.DataFrame = event[event['kind'] == 'word']\n",
    "    raw.annotations.to_data_frame().info()\n",
    "    descs = [json.loads(desc.replace(\"'\", \"\\\"\")) for desc in raw.annotations.description]\n",
    "    starts = [desc['start'] for desc in descs if desc['kind'] == 'word']\n",
    "    sub_id = info['subject_info']['his_id']\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    w_lbs = []\n",
    "    \n",
    "    # word_events.info()\n",
    "    for (i, word_event), start in zip(word_events.iterrows(), starts):\n",
    "        # print(i)\n",
    "        if i >= 100: \n",
    "            break\n",
    "    \n",
    "        duration, word_label, wav_path = word_event['duration'], word_event['word'], word_event['sound'] \n",
    "\n",
    "        wav_path = wav_path.lower()\n",
    "\n",
    "        offset = 0.\n",
    "        start = start + offset\n",
    "        end = start + duration + offset\n",
    "        n_fft = 120\n",
    "        # print(start, end, wav_path, word_label)\n",
    "        t_idxs = raw.time_as_index([start, end])\n",
    "        data, times = raw[:, t_idxs[0]:t_idxs[1]] \n",
    "        if data.shape[-1] < n_fft:\n",
    "           continue\n",
    "\n",
    "        spectrums: Spectrum = raw.compute_psd(tmin=start, tmax=end, fmax=60, n_fft=n_fft)\n",
    "        # spectrums.plot(picks=\"data\", exclude=\"bads\", amplitude=False)\n",
    "        data, freqs = spectrums.get_data(return_freqs=True)\n",
    "        assert len(freqs) == 8\n",
    "        \n",
    "        # print('should have 8 freq bins: ', len(freqs))\n",
    "        # preprocessed_data = to_spectrum(data)\n",
    "        # bands = julius.split_bands(torch.Tensor(data), n_bands=10, sample_rate=120)\n",
    "\n",
    "        audio_label = get_audio_label(wav_path, start, duration)\n",
    "\n",
    "        print(f'word: {word_label}, audio features: {torch.tensor(audio_label).shape}, PSD: {torch.tensor(data).shape}')\n",
    "        x.append(data)\n",
    "        y.append(audio_label)\n",
    "        w_lbs.append(word_label)\n",
    "    # X.append(x)\n",
    "    # Y.append(y)\n",
    "    # word_labels.append(w_lbs)\n",
    "\n",
    "    subs[sub_id].append((x, y, w_lbs))\n",
    "\n",
    "# print(X, Y, word_labels)\n",
    "\n",
    "# for each, start + offset, start + duration + offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = list(subs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tara',\n",
       " 'stood',\n",
       " 'stock',\n",
       " 'still',\n",
       " 'waiting',\n",
       " 'for',\n",
       " 'first',\n",
       " 'tiny',\n",
       " 'gleam',\n",
       " 'from',\n",
       " 'scout',\n",
       " 'craft',\n",
       " 'to',\n",
       " 'appear',\n",
       " 'darkness',\n",
       " 'of',\n",
       " 'The']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs[0][0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from /projects/SilSpeech/Dev/SlientSpeech/LAVIS-main/lavis/datasets/builders/__init__.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukas/miniforge3/envs/lavis-bm3/lib/python3.8/site-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore\n",
      "/home/lukas/miniforge3/envs/lavis-bm3/lib/python3.8/site-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore\n",
      "/home/lukas/projects/LAVIS-main/lavis/models/belt2_models/vq/vector_quantize_pytorch.py:264: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/lukas/projects/LAVIS-main/lavis/models/belt2_models/vq/vector_quantize_pytorch.py:394: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize_weights\n",
      ">>>>>>>>>>>>>>> clf model using base\n",
      "build success\n",
      ">>>>>>>>>>>>>>>forward\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "adaptive_max_pool1d(): Expected input to have non-zero size for non-batch dimensions, but input has sizes [2, 1024, 0] with dimension 2 being empty",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m########################\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# forwad\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>>>>>>>>>>>>>>>forward\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m output:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/lavis-bm3/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/lavis-bm3/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/LAVIS-main/lavis/models/belt3_models/belt_clip_mae.py:4260\u001b[0m, in \u001b[0;36mClip_TemporalConformer2D.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m   4258\u001b[0m eeg_mae_encode \u001b[38;5;241m=\u001b[39m eeg_latent[:, \u001b[38;5;241m1\u001b[39m:, :]\n\u001b[1;32m   4259\u001b[0m \u001b[38;5;66;03m# apply max pooling to get eeg token\u001b[39;00m\n\u001b[0;32m-> 4260\u001b[0m eeg_mae_encode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaxpool\u001b[49m\u001b[43m(\u001b[49m\u001b[43meeg_mae_encode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   4261\u001b[0m \u001b[38;5;66;03m# Getting EEG and Text Features        \u001b[39;00m\n\u001b[1;32m   4262\u001b[0m \u001b[38;5;66;03m# print('eeg_mae_encode',eeg_mae_encode.shape)\u001b[39;00m\n\u001b[1;32m   4263\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4266\u001b[0m \u001b[38;5;66;03m# --------------------Classification Learning---------------------\u001b[39;00m\n\u001b[1;32m   4267\u001b[0m \u001b[38;5;66;03m# print('image_embeddings',image_embeddings.shape,'text_embeddings',text_embeddings.shape)\u001b[39;00m\n\u001b[1;32m   4268\u001b[0m cls_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(eeg_mae_encode\u001b[38;5;241m.\u001b[39mcontiguous())\n",
      "File \u001b[0;32m~/miniforge3/envs/lavis-bm3/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/lavis-bm3/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/lavis-bm3/lib/python3.8/site-packages/torch/nn/modules/pooling.py:1104\u001b[0m, in \u001b[0;36mAdaptiveMaxPool1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[0;32m-> 1104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madaptive_max_pool1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/lavis-bm3/lib/python3.8/site-packages/torch/_jit_internal.py:503\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 503\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/lavis-bm3/lib/python3.8/site-packages/torch/nn/functional.py:1131\u001b[0m, in \u001b[0;36m_adaptive_max_pool1d\u001b[0;34m(input, output_size, return_indices)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   1129\u001b[0m         adaptive_max_pool1d, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, output_size, return_indices\u001b[38;5;241m=\u001b[39mreturn_indices\n\u001b[1;32m   1130\u001b[0m     )\n\u001b[0;32m-> 1131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43madaptive_max_pool1d_with_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/lavis-bm3/lib/python3.8/site-packages/torch/nn/functional.py:1123\u001b[0m, in \u001b[0;36madaptive_max_pool1d_with_indices\u001b[0;34m(input, output_size, return_indices)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   1121\u001b[0m         adaptive_max_pool1d_with_indices, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, output_size, return_indices\u001b[38;5;241m=\u001b[39mreturn_indices\n\u001b[1;32m   1122\u001b[0m     )\n\u001b[0;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madaptive_max_pool1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: adaptive_max_pool1d(): Expected input to have non-zero size for non-batch dimensions, but input has sizes [2, 1024, 0] with dimension 2 being empty"
     ]
    }
   ],
   "source": [
    "from lavis import registry\n",
    "# import lavis.models.belt3_models.belt_clip_mae import Clip_TemporalConformer2D\n",
    "import lavis.models.belt3_models.belt_clip_mae\n",
    "import torch\n",
    "model_cls = registry.get_model_class(\"Temporal-Spatial2D-Conformer\")    \n",
    "# Tara torch.Size([14, 768]) torch.Size([208, 19])\n",
    "batch_size = 2\n",
    "input_sample = {\n",
    "    'eeg': torch.rand(batch_size, 208, 19, 1),\n",
    "    'label': torch.randint(0, 2, (batch_size,)),\n",
    "    \"epoch\": 0,\n",
    "    \"iters\": 0,\n",
    "    \"mask_ratio\": 0.50,\n",
    "}\n",
    "    \n",
    "    # 'pretrain_time_mask', 'pretrain_channel_mask','freq_band_mask_welch_0.5'\n",
    "for mae_type in ['base']:\n",
    "    model = model_cls.from_config(type=mae_type).cpu()\n",
    "    print('>>>>>>>>>>>>>>> clf model using {}'.format(mae_type))\n",
    "    print('build success')\n",
    "    ########################\n",
    "    # forwad\n",
    "    print('>>>>>>>>>>>>>>>forward')\n",
    "    output = model(input_sample)\n",
    "    for k in output:\n",
    "        try:\n",
    "            print(k, output[k].size())\n",
    "        except:\n",
    "            print(k, type(output[k]))\n",
    "    ########################\n",
    "    # generate\n",
    "    print('>>>>>>>>>>>>>>>generate')\n",
    "    output = model.generate(input_sample)\n",
    "    for k in output:\n",
    "        try:\n",
    "            print(k, output[k].size())\n",
    "        except:\n",
    "            print(k, type(output[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 14, 768]) torch.Size([208, 300]) Tara\n",
      "torch.Size([1, 11, 768]) torch.Size([208, 240]) stood\n",
      "torch.Size([1, 18, 768]) torch.Size([208, 370]) stock\n",
      "torch.Size([1, 19, 768]) torch.Size([208, 400]) still\n",
      "torch.Size([1, 20, 768]) torch.Size([208, 410]) waiting\n",
      "torch.Size([1, 6, 768]) torch.Size([208, 130]) for\n",
      "torch.Size([1, 4, 768]) torch.Size([208, 90]) the\n",
      "torch.Size([1, 13, 768]) torch.Size([208, 270]) first\n",
      "torch.Size([1, 15, 768]) torch.Size([208, 310]) tiny\n",
      "torch.Size([1, 11, 768]) torch.Size([208, 240]) gleam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21450/2237729784.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print(torch.tensor(Y[0][i]).shape, torch.tensor(X[0][i]).shape, word_labels[0][i])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print(torch.tensor(Y[0][i]).shape, torch.tensor(X[0][i]).shape, word_labels[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>story_uid</th>\n",
       "      <th>sound_id</th>\n",
       "      <th>kind</th>\n",
       "      <th>start</th>\n",
       "      <th>sound</th>\n",
       "      <th>duration</th>\n",
       "      <th>filepath</th>\n",
       "      <th>phoneme</th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>...</th>\n",
       "      <th>speech_rate</th>\n",
       "      <th>voice</th>\n",
       "      <th>pronounced</th>\n",
       "      <th>word</th>\n",
       "      <th>language</th>\n",
       "      <th>modality</th>\n",
       "      <th>word_sequence</th>\n",
       "      <th>phoneme_id</th>\n",
       "      <th>offset</th>\n",
       "      <th>uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>block</td>\n",
       "      <td>8.245</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.120000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>english</td>\n",
       "      <td>audio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Cable Fort by Bill Glover</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cable_spool_fort</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>word</td>\n",
       "      <td>8.245</td>\n",
       "      <td>stimuli/audio/cable_spool_fort_0.wav</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>160.0</td>\n",
       "      <td>Ava</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The</td>\n",
       "      <td>english</td>\n",
       "      <td>audio</td>\n",
       "      <td>The Cable Fort by Bill Glover</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cable_spool_fort</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sound</td>\n",
       "      <td>8.245</td>\n",
       "      <td>stimuli/audio/cable_spool_fort_0.0.wav</td>\n",
       "      <td>100.597778</td>\n",
       "      <td>/home/lukas/projects/brainmagick/data/gwilliam...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>english</td>\n",
       "      <td>audio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cable_spool_fort</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>phoneme</td>\n",
       "      <td>8.245</td>\n",
       "      <td>stimuli/audio/cable_spool_fort_0.wav</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dh_B</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>160.0</td>\n",
       "      <td>Ava</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>english</td>\n",
       "      <td>audio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cable_spool_fort</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>phoneme</td>\n",
       "      <td>8.295</td>\n",
       "      <td>stimuli/audio/cable_spool_fort_0.wav</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ah_E</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>160.0</td>\n",
       "      <td>Ava</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>english</td>\n",
       "      <td>audio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6729</th>\n",
       "      <td>cable_spool_fort</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>phoneme</td>\n",
       "      <td>700.556</td>\n",
       "      <td>stimuli/audio/cable_spool_fort_5.wav</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l_E</td>\n",
       "      <td>128.0</td>\n",
       "      <td>...</td>\n",
       "      <td>190.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>english</td>\n",
       "      <td>audio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6730</th>\n",
       "      <td>cable_spool_fort</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>word</td>\n",
       "      <td>700.626</td>\n",
       "      <td>stimuli/audio/cable_spool_fort_5.wav</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128.0</td>\n",
       "      <td>...</td>\n",
       "      <td>190.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>right</td>\n",
       "      <td>english</td>\n",
       "      <td>audio</td>\n",
       "      <td>It was all right</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6731</th>\n",
       "      <td>cable_spool_fort</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>phoneme</td>\n",
       "      <td>700.626</td>\n",
       "      <td>stimuli/audio/cable_spool_fort_5.wav</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>r_B</td>\n",
       "      <td>128.0</td>\n",
       "      <td>...</td>\n",
       "      <td>190.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>english</td>\n",
       "      <td>audio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6732</th>\n",
       "      <td>cable_spool_fort</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>phoneme</td>\n",
       "      <td>700.716</td>\n",
       "      <td>stimuli/audio/cable_spool_fort_5.wav</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ay_I</td>\n",
       "      <td>128.0</td>\n",
       "      <td>...</td>\n",
       "      <td>190.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>english</td>\n",
       "      <td>audio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6733</th>\n",
       "      <td>cable_spool_fort</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>phoneme</td>\n",
       "      <td>700.856</td>\n",
       "      <td>stimuli/audio/cable_spool_fort_5.wav</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t_E</td>\n",
       "      <td>128.0</td>\n",
       "      <td>...</td>\n",
       "      <td>190.0</td>\n",
       "      <td>Allison</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>english</td>\n",
       "      <td>audio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6734 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 story  story_uid  sound_id     kind    start  \\\n",
       "0                  NaN        NaN       NaN    block    8.245   \n",
       "1     cable_spool_fort        1.0       0.0     word    8.245   \n",
       "2     cable_spool_fort        1.0       0.0    sound    8.245   \n",
       "3     cable_spool_fort        1.0       0.0  phoneme    8.245   \n",
       "4     cable_spool_fort        1.0       0.0  phoneme    8.295   \n",
       "...                ...        ...       ...      ...      ...   \n",
       "6729  cable_spool_fort        1.0       5.0  phoneme  700.556   \n",
       "6730  cable_spool_fort        1.0       5.0     word  700.626   \n",
       "6731  cable_spool_fort        1.0       5.0  phoneme  700.626   \n",
       "6732  cable_spool_fort        1.0       5.0  phoneme  700.716   \n",
       "6733  cable_spool_fort        1.0       5.0  phoneme  700.856   \n",
       "\n",
       "                                       sound    duration  \\\n",
       "0                                        NaN    3.120000   \n",
       "1       stimuli/audio/cable_spool_fort_0.wav    0.130000   \n",
       "2     stimuli/audio/cable_spool_fort_0.0.wav  100.597778   \n",
       "3       stimuli/audio/cable_spool_fort_0.wav    0.050000   \n",
       "4       stimuli/audio/cable_spool_fort_0.wav    0.080000   \n",
       "...                                      ...         ...   \n",
       "6729    stimuli/audio/cable_spool_fort_5.wav    0.070000   \n",
       "6730    stimuli/audio/cable_spool_fort_5.wav    0.430000   \n",
       "6731    stimuli/audio/cable_spool_fort_5.wav    0.090000   \n",
       "6732    stimuli/audio/cable_spool_fort_5.wav    0.140000   \n",
       "6733    stimuli/audio/cable_spool_fort_5.wav    0.200000   \n",
       "\n",
       "                                               filepath phoneme  sequence_id  \\\n",
       "0                                                   NaN     NaN          NaN   \n",
       "1                                                   NaN     NaN          0.0   \n",
       "2     /home/lukas/projects/brainmagick/data/gwilliam...     NaN          NaN   \n",
       "3                                                   NaN    dh_B          0.0   \n",
       "4                                                   NaN    ah_E          0.0   \n",
       "...                                                 ...     ...          ...   \n",
       "6729                                                NaN     l_E        128.0   \n",
       "6730                                                NaN     NaN        128.0   \n",
       "6731                                                NaN     r_B        128.0   \n",
       "6732                                                NaN    ay_I        128.0   \n",
       "6733                                                NaN     t_E        128.0   \n",
       "\n",
       "      ... speech_rate    voice  pronounced   word  language modality  \\\n",
       "0     ...         NaN      NaN         NaN    NaN   english    audio   \n",
       "1     ...       160.0      Ava         1.0    The   english    audio   \n",
       "2     ...         NaN      NaN         NaN    NaN   english    audio   \n",
       "3     ...       160.0      Ava         1.0    NaN   english    audio   \n",
       "4     ...       160.0      Ava         1.0    NaN   english    audio   \n",
       "...   ...         ...      ...         ...    ...       ...      ...   \n",
       "6729  ...       190.0  Allison         1.0    NaN   english    audio   \n",
       "6730  ...       190.0  Allison         1.0  right   english    audio   \n",
       "6731  ...       190.0  Allison         1.0    NaN   english    audio   \n",
       "6732  ...       190.0  Allison         1.0    NaN   english    audio   \n",
       "6733  ...       190.0  Allison         1.0    NaN   english    audio   \n",
       "\n",
       "                      word_sequence phoneme_id offset  \\\n",
       "0                               NaN        NaN    NaN   \n",
       "1     The Cable Fort by Bill Glover        NaN    NaN   \n",
       "2                               NaN        NaN    0.0   \n",
       "3                               NaN        0.0    NaN   \n",
       "4                               NaN        1.0    NaN   \n",
       "...                             ...        ...    ...   \n",
       "6729                            NaN        1.0    NaN   \n",
       "6730               It was all right        NaN    NaN   \n",
       "6731                            NaN        0.0    NaN   \n",
       "6732                            NaN        1.0    NaN   \n",
       "6733                            NaN        2.0    NaN   \n",
       "\n",
       "                                uid  \n",
       "0     The Cable Fort by Bill Glover  \n",
       "1                               NaN  \n",
       "2                               NaN  \n",
       "3                               NaN  \n",
       "4                               NaN  \n",
       "...                             ...  \n",
       "6729                            NaN  \n",
       "6730                            NaN  \n",
       "6731                            NaN  \n",
       "6732                            NaN  \n",
       "6733                            NaN  \n",
       "\n",
       "[6734 rows x 22 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hostname lukas-B550-AORUS-ELITE-AX-V2 not defined in /conf/study_paths/study_paths.yaml. Using default paths.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wandb': {'use_wandb': False, 'project': 'brainmagick', 'group': 'brainmagick-group'}, 'num_prints': 5, 'device': 'cuda', 'num_workers': 5, 'verbose': 0, 'show': 0, 'download_only': False, 'slurm': {'mem_per_gpu': 100, 'time': 4320}, 'continue_sig': None, 'continue_best': True, 'seed': 2036, 'dummy': None, 'cache': './cache', 'features_models': './features_models', 'early_stop_patience': 10, 'eval_every': 1, 'eval_train_set': False, 'optim': {'name': 'adam', 'lr': 0.0003, 'beta2': 0.999, 'epochs': 200, 'batch_size': 64, 'loss': 'clip', 'use_weighting': False, 'max_batches': 1200, 'svd': 0.0, 'negatives': None, 'negative_pool_size': None}, 'clip': {'linear': None, 'twin': True, 'pool': False, 'tmin': None, 'tmax': None, 'tmin_train': None, 'tmax_train': None, 'center': False}, 'test': {'wer_negatives': 10000, 'wer_topx': 10, 'wer_random': False, 'wer_recordings': 40, 'wer_study': None}, 'dset': {'selections': ['gwilliams2022'], 'tmin': -0.5, 'tmax': 2.5, 'n_recordings': 4, 'n_subjects': None, 'n_subjects_test': None, 'shuffle_recordings_seed': -1, 'skip_recordings': 0, 'test_ratio': 0.2, 'valid_ratio': 0.1, 'remove_ratio': 0.0, 'condition': 0.5, 'apply_baseline': True, 'min_block_duration': 6, 'force_uid_assignement': False, 'min_n_blocks_per_split': 1, 'ignore_end_in_block': False, 'ignore_start_in_block': False, 'sample_rate': 120, 'highpass': 0, 'event_mask': True, 'split_wav_as_block': True, 'allow_empty_split': False, 'autoreject': False, 'test': {'tmin': None, 'tmax': None, 'condition': 'word'}, 'features': ['Wav2VecTransformer'], 'extra_test_features': [], 'features_params': {'MelSpectrum': {'n_fft': 512, 'n_mels': 120, 'normalized': True, 'use_log_scale': True, 'log_scale_eps': 1e-05}, 'Pitch': {'min_f0': 100, 'max_f0': 350}, 'WordHash': {'buckets': 100000}, 'XlmEmbedding': {'contextual': False}, 'WordEmbedding': {'lang': 'auto'}, 'WordEmbeddingSmall': {'lang': 'auto'}, 'PartOfSpeech': {'lang': 'auto'}, 'Wav2VecTransformer': {'layers': [14, 15, 16, 17, 18], 'device': 'cpu', 'random': False}, 'Wav2VecChunk': {'device': 'cpu'}}}, 'override_n_subjects_model': None, 'norm': {'scaler': {'per_channel': False, 'n_samples_per_recording': 200, 'n_samples_features': 8000}, 'max_scale': 20.0, 'clip': True, 'exclude_empty_features': False}, 'task': {'type': 'decode', 'meg_init': 0.3, 'lowpass': 0, 'offset_meg_ms': 150, 'lowpass_gt': True, 'lowpass_gt_test': False, 'mask_loss': False}, 'dora': {'dir': './outputs', 'exclude': ['wandb.*', 'num_prints', 'device', 'num_workers', 'verbose', 'cache', 'features_models'], 'git_save': True}, 'model_name': 'simpleconv', 'convrnn': {'concatenate': False, 'depth': 2, 'linear_out': False, 'complex_out': False, 'kernel_size': 4, 'stride': 2, 'growth': 1.0, 'lstm': 4, 'bidirectional_lstm': False, 'flip_lstm': False, 'attention': 0, 'heads': 4, 'conv_dropout': 0.0, 'lstm_dropout': 0.0, 'dropout_input': 0.0, 'batch_norm': False, 'relu_leakiness': 0.0, 'subject_dim': 64, 'embedding_location': ['lstm'], 'embedding_scale': 1.0, 'subject_layers': False, 'subject_layers_dim': 'input'}, 'simpleconv': {'concatenate': False, 'depth': 10, 'linear_out': False, 'complex_out': True, 'kernel_size': 3, 'dilation_growth': 2, 'dilation_period': 5, 'skip': True, 'post_skip': False, 'growth': 1.0, 'scale': None, 'rewrite': False, 'groups': 1, 'glu': 2, 'glu_context': 1, 'glu_glu': True, 'gelu': True, 'dual_path': 0, 'conv_dropout': 0.0, 'dropout_input': 0.0, 'batch_norm': True, 'relu_leakiness': 0.0, 'subject_dim': 0, 'subject_layers': True, 'embedding_scale': 1.0, 'subject_layers_dim': 'input', 'subject_layers_id': False, 'n_fft': None, 'fft_complex': True, 'merger': True, 'merger_pos_dim': 2048, 'merger_channels': 270, 'merger_dropout': 0.2, 'merger_penalty': 0.0, 'merger_per_subject': False, 'dropout': 0.0, 'dropout_rescale': True, 'initial_linear': 270, 'initial_depth': 1, 'initial_nonlin': False, 'hidden': {'meg': 320}}, 'feature_model_name': None, 'selections': {'audio_mous': {'study': 'schoffelen2019', 'modality': 'audio', 'events_filter': None}, 'audio_mous_wl': {'study': 'schoffelen2019', 'modality': 'audio', 'events_filter': 'condition == \"word_list\"'}, 'visual_mous': {'study': 'schoffelen2019', 'modality': 'visual', 'events_filter': None}, 'gwilliams2022': {'study': 'gwilliams2022'}, 'broderick2019': {'study': 'broderick2019'}, 'fake': {'study': 'fake'}, 'brennan2019': {'study': 'brennan2019'}}, 'study_paths': {'default': {'gwilliams2022': './data/gwilliams2022/', 'schoffelen2019': './data/schoffelen2019/', 'brennan2019': './data/brennan2019/', 'broderick2019': './data/broderick2019/'}}}\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m     os\u001b[38;5;241m.\u001b[39mchdir(Path(bm\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m)\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(cfg)\n\u001b[0;32m---> 21\u001b[0m     raw, events \u001b[38;5;241m=\u001b[39m \u001b[43mmeta_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain_nb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# sys.argv = [sys.argv[0]]\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# initialize(config_path=\"conf\")\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# cfg = compose(config_name=\"config.yaml\")\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# raw, events = meta_dataset.main()\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/brainmagick/bm/meta_dataset.py:141\u001b[0m, in \u001b[0;36mmain_nb\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain_nb\u001b[39m(args: tp\u001b[38;5;241m.\u001b[39mAny):\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmain_\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/brainmagick/bm/meta_dataset.py:157\u001b[0m, in \u001b[0;36mmain_\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    155\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCaching intermediate data under \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mcache\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(args)\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_BM_TEST_PATH\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    161\u001b[0m     main\u001b[38;5;241m.\u001b[39mdora\u001b[38;5;241m.\u001b[39mdir \u001b[38;5;241m=\u001b[39m Path(os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_BM_TEST_PATH\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/projects/brainmagick/bm/meta_dataset.py:133\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    131\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextra_test_features\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWordHash\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/brainmagick/bm/meta_dataset.py:63\u001b[0m, in \u001b[0;36mget_datasets\u001b[0;34m(selections, n_recordings, test_ratio, valid_ratio, sample_rate, highpass, num_workers, apply_baseline, progress, skip_recordings, min_block_duration, force_uid_assignement, shuffle_recordings_seed, split_assign_seed, min_n_blocks_per_split, features, extra_test_features, test, allow_empty_split, n_subjects, n_subjects_test, remove_ratio, **factory_kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_datasets\u001b[39m(selections: tp\u001b[38;5;241m.\u001b[39mList[tp\u001b[38;5;241m.\u001b[39mDict[\u001b[38;5;28mstr\u001b[39m, tp\u001b[38;5;241m.\u001b[39mAny]],\n\u001b[1;32m     38\u001b[0m         n_recordings: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m     39\u001b[0m         test_ratio: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m \n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# get from running gwilliams study\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m     all_recordings \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_recordings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mselections\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_recordings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_recordings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_recordings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle_recordings_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle_recordings_seed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     all_recordings \u001b[38;5;241m=\u001b[39m LogProgress(logger, all_recordings,\n\u001b[1;32m     67\u001b[0m                                       name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreparing cache\u001b[39m\u001b[38;5;124m\"\u001b[39m, level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mDEBUG)\n\u001b[1;32m     68\u001b[0m     all_recordings \u001b[38;5;241m=\u001b[39m [  \u001b[38;5;66;03m# for debugging\u001b[39;00m\n\u001b[1;32m     69\u001b[0m         _preload(s, sample_rate\u001b[38;5;241m=\u001b[39msample_rate, highpass\u001b[38;5;241m=\u001b[39mhighpass) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m all_recordings]\n",
      "File \u001b[0;32m~/projects/brainmagick/bm/dataset.py:391\u001b[0m, in \u001b[0;36m_extract_recordings\u001b[0;34m(selections, n_recordings, skip_recordings, shuffle_recordings_seed)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_extract_recordings\u001b[39m(selections: tp\u001b[38;5;241m.\u001b[39mList[tp\u001b[38;5;241m.\u001b[39mDict[\u001b[38;5;28mstr\u001b[39m, tp\u001b[38;5;241m.\u001b[39mAny]], n_recordings: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m    387\u001b[0m                         skip_recordings: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, shuffle_recordings_seed: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    388\u001b[0m                         ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m tp\u001b[38;5;241m.\u001b[39mSequence[studies\u001b[38;5;241m.\u001b[39mRecording]:\n\u001b[1;32m    389\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Extract the number of recordings required, and mix audio and visual if need be\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m  \u001b[38;5;66;03m# this is a function to help testing, especially the \"any\" case\u001b[39;00m\n\u001b[0;32m--> 391\u001b[0m     recording_lists \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(studies\u001b[38;5;241m.\u001b[39mfrom_selection(select)) \u001b[38;5;28;01mfor\u001b[39;00m select \u001b[38;5;129;01min\u001b[39;00m selections]\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle_recordings_seed \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# deactivated if -1\u001b[39;00m\n\u001b[1;32m    393\u001b[0m         rng \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mRandomState(seed\u001b[38;5;241m=\u001b[39mshuffle_recordings_seed)\n",
      "File \u001b[0;32m~/projects/brainmagick/bm/dataset.py:391\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_extract_recordings\u001b[39m(selections: tp\u001b[38;5;241m.\u001b[39mList[tp\u001b[38;5;241m.\u001b[39mDict[\u001b[38;5;28mstr\u001b[39m, tp\u001b[38;5;241m.\u001b[39mAny]], n_recordings: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m    387\u001b[0m                         skip_recordings: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, shuffle_recordings_seed: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    388\u001b[0m                         ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m tp\u001b[38;5;241m.\u001b[39mSequence[studies\u001b[38;5;241m.\u001b[39mRecording]:\n\u001b[1;32m    389\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Extract the number of recordings required, and mix audio and visual if need be\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m  \u001b[38;5;66;03m# this is a function to help testing, especially the \"any\" case\u001b[39;00m\n\u001b[0;32m--> 391\u001b[0m     recording_lists \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstudies\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m select \u001b[38;5;129;01min\u001b[39;00m selections]\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle_recordings_seed \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# deactivated if -1\u001b[39;00m\n\u001b[1;32m    393\u001b[0m         rng \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mRandomState(seed\u001b[38;5;241m=\u001b[39mshuffle_recordings_seed)\n",
      "File \u001b[0;32m~/projects/brainmagick/bm/studies/gwilliams2022.py:63\u001b[0m, in \u001b[0;36mGwilliams2022Recording.iter\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a generator of all recordings\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# download, extract, organize\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# List all recordings: depends on study structure\u001b[39;00m\n\u001b[1;32m     65\u001b[0m paths \u001b[38;5;241m=\u001b[39m StudyPaths()\n",
      "File \u001b[0;32m~/projects/brainmagick/bm/studies/gwilliams2022.py:54\u001b[0m, in \u001b[0;36mGwilliams2022Recording.download\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload\u001b[39m(\u001b[38;5;28mcls\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[43mdownload_osf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mag3kj\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mStudyPaths\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mag3kj\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     download_osf(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh2tzn\u001b[39m\u001b[38;5;124m'\u001b[39m, StudyPaths()\u001b[38;5;241m.\u001b[39mdownload\u001b[38;5;241m.\u001b[39mparent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh2tzn\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     56\u001b[0m     download_osf(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mu5327\u001b[39m\u001b[38;5;124m'\u001b[39m, StudyPaths()\u001b[38;5;241m.\u001b[39mdownload\u001b[38;5;241m.\u001b[39mparent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mu5327\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/projects/brainmagick/bm/studies/download.py:21\u001b[0m, in \u001b[0;36mdownload_osf\u001b[0;34m(study, dset_dir, success)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m dset_dir \u001b[38;5;241m=\u001b[39m to_absolute_path(Path(dset_dir))\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m dset_dir\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mexists()\n\u001b[1;32m     22\u001b[0m dl_dir \u001b[38;5;241m=\u001b[39m dset_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdownload\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m success_file \u001b[38;5;241m=\u001b[39m dl_dir \u001b[38;5;241m/\u001b[39m success\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import hydra.core.global_hydra\n",
    "import bm\n",
    "from bm import meta_dataset\n",
    "import sys\n",
    "import os\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose, core\n",
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "from pathlib import Path\n",
    "import inspect\n",
    "\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "\n",
    "with initialize(version_base=\"1.1\", config_path=\"../bm/conf\"):\n",
    "    cfg = compose(config_name=\"config.yaml\")\n",
    "    os.chdir(Path(bm.__file__).parent)\n",
    "    print(cfg)\n",
    "    raw, events = meta_dataset.main_nb(cfg)\n",
    "\n",
    "\n",
    "\n",
    "# sys.argv = [sys.argv[0]]\n",
    "# initialize(config_path=\"conf\")\n",
    "# cfg = compose(config_name=\"config.yaml\")\n",
    "\n",
    "# raw, events = meta_dataset.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
